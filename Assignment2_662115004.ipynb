{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38008395",
   "metadata": {},
   "source": [
    "# NLP Programming Assignment 2\n",
    "\n",
    "### Jirapat Sereerat 662115004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037338f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Jirapat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Jirapat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Jirapat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Jirapat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Jirapat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loaded. Rows: 1048575\n"
     ]
    }
   ],
   "source": [
    "# !pip install textstat\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import nltk\n",
    "import textstat\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "file_path = 'Sentiment Analysis Dataset.csv'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(file_path, encoding='latin-1')\n",
    "except:\n",
    "    df = pd.read_csv(file_path, encoding='latin-1', error_bad_lines=False)\n",
    "\n",
    "print(f\"Dataset Loaded. Rows: {len(df)}\")\n",
    "\n",
    "# Start Timer\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c67babc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ItemID  Sentiment                                      SentimentText\n",
       "0       1          0                       is so sad for my APL frie...\n",
       "1       2          0                     I missed the New Moon trail...\n",
       "2       3          1                            omg its already 7:30 :O\n",
       "3       4          0            .. Omgaga. Im sooo  im gunna CRy. I'...\n",
       "4       5          0           i think mi bf is cheating on me!!!   ..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c762c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 (Cleaning) Complete.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SentimentText</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "      <td>is so sad for my APL friend.............</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "      <td>I missed the New Moon trailer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "      <td>omg its already O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "      <td>.. Omgaga. Im sooo im gunna CRy. I've been at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "      <td>i think mi bf is cheating on me!!! TT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       SentimentText  \\\n",
       "0                       is so sad for my APL frie...   \n",
       "1                     I missed the New Moon trail...   \n",
       "2                            omg its already 7:30 :O   \n",
       "3            .. Omgaga. Im sooo  im gunna CRy. I'...   \n",
       "4           i think mi bf is cheating on me!!!   ...   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0           is so sad for my APL friend.............  \n",
       "1                   I missed the New Moon trailer...  \n",
       "2                                  omg its already O  \n",
       "3  .. Omgaga. Im sooo im gunna CRy. I've been at ...  \n",
       "4              i think mi bf is cheating on me!!! TT  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# STEP 1: TEXT CLEANING\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Regex Patterns\n",
    "emoticon_pattern = r'(?::|;|=)(?:-)?(?:\\)|\\(|D|P|\\|)'\n",
    "phone_pattern = r'\\(?\\b[0-9]{3}\\)?[-. ]?[0-9]{3}[-. ]?[0-9]{4}\\b'\n",
    "email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
    "account_pattern = r'\\b\\d{8,16}\\b'\n",
    "\n",
    "stats = {\n",
    "    'emoticon_removed': 0,\n",
    "    'special_char_removed': 0,\n",
    "    'phone_count': 0,\n",
    "    'email_count': 0,\n",
    "    'account_count': 0\n",
    "}\n",
    "\n",
    "def clean_text_step1(text):\n",
    "    text = str(text) \n",
    "    \n",
    "    # 1.6 PII Counting (Before cleaning)\n",
    "    stats['phone_count'] += len(re.findall(phone_pattern, text))\n",
    "    stats['email_count'] += len(re.findall(email_pattern, text))\n",
    "    stats['account_count'] += len(re.findall(account_pattern, text))\n",
    "    \n",
    "    # 1.5 Emoticon Counting & Removal\n",
    "    emoticons = re.findall(emoticon_pattern, text)\n",
    "    stats['emoticon_removed'] += len(emoticons)\n",
    "    # Remove emoticons \n",
    "    text = re.sub(emoticon_pattern, '', text)\n",
    "    \n",
    "    # Track length before special char removal\n",
    "    len_before = len(text)\n",
    "    \n",
    "    # Remove Special Characters & Numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s.,!?\\'\"]', '', text)\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Calculate removed special chars\n",
    "    stats['special_char_removed'] += (len_before - len(text))\n",
    "    \n",
    "    return text\n",
    "\n",
    "df['cleaned_text'] = df['SentimentText'].apply(clean_text_step1)\n",
    "print(\"Step 1 (Cleaning) Complete.\")\n",
    "df[['SentimentText', 'cleaned_text']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd7355d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2 (Tokenization) Complete.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>raw_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>is so sad for my APL friend.............</td>\n",
       "      <td>[is, so, sad, for, my, APL, friend, .............</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I missed the New Moon trailer...</td>\n",
       "      <td>[I, missed, the, New, Moon, trailer, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>omg its already O</td>\n",
       "      <td>[omg, its, already, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>.. Omgaga. Im sooo im gunna CRy. I've been at ...</td>\n",
       "      <td>[.., Omgaga, ., Im, sooo, im, gunna, CRy, ., I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i think mi bf is cheating on me!!! TT</td>\n",
       "      <td>[i, think, mi, bf, is, cheating, on, me, !, !,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        cleaned_text  \\\n",
       "0           is so sad for my APL friend.............   \n",
       "1                   I missed the New Moon trailer...   \n",
       "2                                  omg its already O   \n",
       "3  .. Omgaga. Im sooo im gunna CRy. I've been at ...   \n",
       "4              i think mi bf is cheating on me!!! TT   \n",
       "\n",
       "                                          raw_tokens  \n",
       "0  [is, so, sad, for, my, APL, friend, .............  \n",
       "1          [I, missed, the, New, Moon, trailer, ...]  \n",
       "2                             [omg, its, already, O]  \n",
       "3  [.., Omgaga, ., Im, sooo, im, gunna, CRy, ., I...  \n",
       "4  [i, think, mi, bf, is, cheating, on, me, !, !,...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# STEP 2: TOKENIZATION\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "def step2_tokenize(text):\n",
    "    # Split into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    # Split into words (for each sentence)\n",
    "    words = word_tokenize(text)\n",
    "    return sentences, words\n",
    "\n",
    "# We will create a temp column to hold the tuple (sentences, words)\n",
    "token_data = df['cleaned_text'].apply(step2_tokenize)\n",
    "\n",
    "df['sentences'] = token_data.apply(lambda x: x[0])\n",
    "df['raw_tokens'] = token_data.apply(lambda x: x[1])\n",
    "\n",
    "print(\"Step 2 (Tokenization) Complete.\")\n",
    "df[['cleaned_text', 'raw_tokens']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dcec85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3 (Lowercasing & Stopwords) Complete.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens_step3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[sad, apl, friend]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[missed, new, moon, trailer]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[omg, already]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[omgaga, im, sooo, im, gunna, cry, dentist, si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[think, mi, bf, cheating, tt]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        tokens_step3\n",
       "0                                 [sad, apl, friend]\n",
       "1                       [missed, new, moon, trailer]\n",
       "2                                     [omg, already]\n",
       "3  [omgaga, im, sooo, im, gunna, cry, dentist, si...\n",
       "4                      [think, mi, bf, cheating, tt]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# STEP 3: LOWERCASING & STOP WORDS\n",
    "# ---------------------------------------------------------\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Counter for stats\n",
    "stats['lowercase_count'] = 0\n",
    "stats['stop_word_removed'] = 0\n",
    "\n",
    "def step3_processing(tokens_list):\n",
    "    processed_tokens = []\n",
    "    \n",
    "    for word in tokens_list:\n",
    "        # Lowercasing Count (Count uppercase chars before converting)\n",
    "        stats['lowercase_count'] += sum(1 for c in word if c.isupper())\n",
    "        \n",
    "        # Convert to lowercase\n",
    "        lower_word = word.lower()\n",
    "        \n",
    "        # Stop Word Removal\n",
    "        if lower_word not in stop_words and lower_word.isalnum():\n",
    "            processed_tokens.append(lower_word)\n",
    "        else:\n",
    "            if lower_word in stop_words:\n",
    "                stats['stop_word_removed'] += 1\n",
    "                \n",
    "    return processed_tokens\n",
    "\n",
    "df['tokens_step3'] = df['raw_tokens'].apply(step3_processing)\n",
    "\n",
    "print(\"Step 3 (Lowercasing & Stopwords) Complete.\")\n",
    "df[['tokens_step3']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54980655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4 (Stemming & Lemmatization) Complete.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens_step3</th>\n",
       "      <th>final_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[sad, apl, friend]</td>\n",
       "      <td>[sad, apl, friend]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[missed, new, moon, trailer]</td>\n",
       "      <td>[miss, new, moon, trailer]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[omg, already]</td>\n",
       "      <td>[omg, alreadi]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[omgaga, im, sooo, im, gunna, cry, dentist, si...</td>\n",
       "      <td>[omgaga, im, sooo, im, gunna, cri, dentist, si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[think, mi, bf, cheating, tt]</td>\n",
       "      <td>[think, mi, bf, cheat, tt]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        tokens_step3  \\\n",
       "0                                 [sad, apl, friend]   \n",
       "1                       [missed, new, moon, trailer]   \n",
       "2                                     [omg, already]   \n",
       "3  [omgaga, im, sooo, im, gunna, cry, dentist, si...   \n",
       "4                      [think, mi, bf, cheating, tt]   \n",
       "\n",
       "                                        final_tokens  \n",
       "0                                 [sad, apl, friend]  \n",
       "1                         [miss, new, moon, trailer]  \n",
       "2                                     [omg, alreadi]  \n",
       "3  [omgaga, im, sooo, im, gunna, cri, dentist, si...  \n",
       "4                         [think, mi, bf, cheat, tt]  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# STEP 4: STEMMING & LEMMATIZATION\n",
    "# ---------------------------------------------------------\n",
    "ps = PorterStemmer()\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def step4_stem_lemma(tokens_list):\n",
    "    # Apply Lemmatization THEN Stemming\n",
    "    return [ps.stem(wnl.lemmatize(w)) for w in tokens_list]\n",
    "\n",
    "df['final_tokens'] = df['tokens_step3'].apply(step4_stem_lemma)\n",
    "\n",
    "print(\"Step 4 (Stemming & Lemmatization) Complete.\")\n",
    "df[['tokens_step3', 'final_tokens']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e6245cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Metrics...\n",
      "\n",
      "========================================\n",
      "           FINAL REPORT\n",
      "========================================\n",
      "1.1 Avg Sentence Length:\n",
      "    Before: 7.63\n",
      "    After:  4.32\n",
      "\n",
      "1.2 Word and Sentence Count:\n",
      "    Words:     13833637.0 (Before) -> 7855752.0 (After)\n",
      "    Sentences: 1814099.0 (Before) -> 1817565.0 (After)\n",
      "\n",
      "1.3 Vocabulary Size:\n",
      "    Before: 696811\n",
      "    After:  479711\n",
      "\n",
      "1.4 Length Stats:\n",
      "    Max Word Length: 117.0 (Before) -> 123.0 (After)\n",
      "    Avg Min Sentence Length (Before): 9.11\n",
      "    Avg Max Sentence Length (Before): 12.84\n",
      "\n",
      "1.5 Removal Counts:\n",
      "    Emoticons Removed:     11753\n",
      "    Stop Words Removed:    5883938\n",
      "    Special Chars Removed: 3606557\n",
      "    Lowercase Conversions: 3452053\n",
      "\n",
      "1.6 PII Counts:\n",
      "    Emails: 417\n",
      "    Phones: 363\n",
      "    Accounts: 550\n",
      "\n",
      "2. Automated Metrics:\n",
      "    Readability (Flesch) Before: 77.71\n",
      "    Readability (Flesch) After:  72.42\n",
      "    Lexical Diversity Before:    0.96\n",
      "    Lexical Diversity After:     0.98\n",
      "\n",
      "3. Runtime:\n",
      "    Total Time: 867.6703 seconds\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# STATISTICS & METRICS\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "def calculate_row_stats(row):\n",
    "\n",
    "    # Before\n",
    "    sents_before = sent_tokenize(str(row['SentimentText']))\n",
    "    words_before = [w for w in word_tokenize(str(row['SentimentText'])) if w.isalnum()]\n",
    "    \n",
    "    # After (Using final tokens and cleaned sentences count)\n",
    "    sents_after = row['sentences']\n",
    "    words_after = row['final_tokens']\n",
    "    \n",
    "    # Min/Max Sent Len Before\n",
    "    sent_lens_b = [len(word_tokenize(s)) for s in sents_before]\n",
    "    min_sent_b = min(sent_lens_b) if sent_lens_b else 0\n",
    "    max_sent_b = max(sent_lens_b) if sent_lens_b else 0\n",
    "    \n",
    "    # Min/Max Sent Len After\n",
    "    avg_len_after = len(words_after) / len(sents_after) if sents_after else 0\n",
    "    \n",
    "    # 2.1 Readability\n",
    "    flesch_before = textstat.flesch_reading_ease(str(row['SentimentText']))\n",
    "    flesch_after = textstat.flesch_reading_ease(\" \".join(words_after))\n",
    "    \n",
    "    # 2.2 Lexical Diversity\n",
    "    lex_div_before = len(set(words_before)) / len(words_before) if words_before else 0\n",
    "    lex_div_after = len(set(words_after)) / len(words_after) if words_after else 0\n",
    "    \n",
    "    return pd.Series([\n",
    "        len(words_before), len(words_after),       # Word Counts\n",
    "        len(sents_before), len(sents_after),       # Sentence Counts\n",
    "        max([len(w) for w in words_before]) if words_before else 0, # Max Word Len Before\n",
    "        max([len(w) for w in words_after]) if words_after else 0,   # Max Word Len After\n",
    "        min_sent_b, max_sent_b,                    # Min/Max Sent Len Before\n",
    "        avg_len_after,                             # Avg Sent Len After\n",
    "        flesch_before, flesch_after,               # Readability\n",
    "        lex_div_before, lex_div_after              # Lexical Diversity\n",
    "    ])\n",
    "\n",
    "print(\"Calculating Metrics...\")\n",
    "metrics_df = df.apply(calculate_row_stats, axis=1)\n",
    "metrics_df.columns = [\n",
    "    'words_b', 'words_a', 'sents_b', 'sents_a', \n",
    "    'max_word_b', 'max_word_a', 'min_sent_b', 'max_sent_b', 'avg_sent_a',\n",
    "    'flesch_b', 'flesch_a', 'lex_div_b', 'lex_div_a'\n",
    "]\n",
    "\n",
    "# 1.1 Average Sentence Length\n",
    "avg_sent_len_before = metrics_df['words_b'].sum() / metrics_df['sents_b'].sum()\n",
    "avg_sent_len_after = metrics_df['words_a'].sum() / metrics_df['sents_a'].sum()\n",
    "\n",
    "# 1.3 Vocabulary Size \n",
    "vocab_before = set()\n",
    "df['SentimentText'].apply(lambda x: vocab_before.update(word_tokenize(str(x))))\n",
    "vocab_after = set()\n",
    "df['final_tokens'].apply(lambda x: vocab_after.update(x))\n",
    "\n",
    "end_time = time.time()\n",
    "runtime = end_time - start_time\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"           FINAL REPORT\")\n",
    "print(\"=\"*40)\n",
    "print(f\"1.1 Avg Sentence Length:\")\n",
    "print(f\"    Before: {avg_sent_len_before:.2f}\")\n",
    "print(f\"    After:  {avg_sent_len_after:.2f}\")\n",
    "\n",
    "print(f\"\\n1.2 Word and Sentence Count:\")\n",
    "print(f\"    Words:     {metrics_df['words_b'].sum()} (Before) -> {metrics_df['words_a'].sum()} (After)\")\n",
    "print(f\"    Sentences: {metrics_df['sents_b'].sum()} (Before) -> {metrics_df['sents_a'].sum()} (After)\")\n",
    "\n",
    "print(f\"\\n1.3 Vocabulary Size:\")\n",
    "print(f\"    Before: {len(vocab_before)}\")\n",
    "print(f\"    After:  {len(vocab_after)}\")\n",
    "\n",
    "print(f\"\\n1.4 Length Stats:\")\n",
    "print(f\"    Max Word Length: {metrics_df['max_word_b'].max()} (Before) -> {metrics_df['max_word_a'].max()} (After)\")\n",
    "print(f\"    Avg Min Sentence Length (Before): {metrics_df['min_sent_b'].mean():.2f}\")\n",
    "print(f\"    Avg Max Sentence Length (Before): {metrics_df['max_sent_b'].mean():.2f}\")\n",
    "\n",
    "print(f\"\\n1.5 Removal Counts:\")\n",
    "print(f\"    Emoticons Removed:     {stats['emoticon_removed']}\")\n",
    "print(f\"    Stop Words Removed:    {stats['stop_word_removed']}\")\n",
    "print(f\"    Special Chars Removed: {stats['special_char_removed']}\")\n",
    "print(f\"    Lowercase Conversions: {stats['lowercase_count']}\")\n",
    "\n",
    "print(f\"\\n1.6 PII Counts:\")\n",
    "print(f\"    Emails: {stats['email_count']}\")\n",
    "print(f\"    Phones: {stats['phone_count']}\")\n",
    "print(f\"    Accounts: {stats['account_count']}\")\n",
    "\n",
    "print(f\"\\n2. Automated Metrics:\")\n",
    "print(f\"    Readability (Flesch) Before: {metrics_df['flesch_b'].mean():.2f}\")\n",
    "print(f\"    Readability (Flesch) After:  {metrics_df['flesch_a'].mean():.2f}\")\n",
    "print(f\"    Lexical Diversity Before:    {metrics_df['lex_div_b'].mean():.2f}\")\n",
    "print(f\"    Lexical Diversity After:     {metrics_df['lex_div_a'].mean():.2f}\")\n",
    "\n",
    "print(f\"\\n3. Runtime:\")\n",
    "print(f\"    Total Time: {runtime:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9c13d5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[DONE] Processed data saved to 'Processed_Sentiment_Dataset.csv'\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# SAVE PROCESSED DATA\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Create a final dataframe\n",
    "output_df = pd.DataFrame({\n",
    "    'original_text': df['SentimentText'],\n",
    "    'cleaned_text': df['cleaned_text'],\n",
    "    'final_tokens': df['final_tokens'].apply(lambda x: \" \".join(x)),\n",
    "    'readability_original': metrics_df['flesch_b'],\n",
    "    'readability_cleaned': metrics_df['flesch_a']\n",
    "})\n",
    "\n",
    "output_filename = 'Processed_Sentiment_Dataset.csv'\n",
    "output_df.to_csv(output_filename, index=False)\n",
    "\n",
    "print(f\"\\n[DONE] Processed data saved to '{output_filename}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
